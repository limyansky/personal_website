<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Brent Limyansky, PhD</title>
    <description>A website to showcase my personal projects.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 06 Feb 2024 12:55:28 -0500</pubDate>
    <lastBuildDate>Tue, 06 Feb 2024 12:55:28 -0500</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Exploring My Lifetime Spotify History</title>
        <description>&lt;p&gt;I went out to dinner with some close friends the other night, when the topic of music came up. 
We spent some time reminiscing about what we saw as the defining stages of our lives, and how our music tastes changed and developed along with us.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://public.tableau.com/app/profile/brent.limyansky/viz/MySpotifyData_17066362308130/Dashboard12&quot;&gt;Tableau Public&lt;/a&gt;
&lt;a href=&quot;https://github.com/limyansky/my_spotify&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;the-best-things-i-learned&quot;&gt;The Best Things I Learned&lt;/h1&gt;
&lt;p&gt;My single favorite takeaway from this project was seeing how steeply my listening dropped off when I was working on my Master’s Degree.
This was definitely the most social part of grad school for me, where we were either TAing or working on homework problems together nearly all the time.
Apparently I wasn’t selected to be DJ…&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/assets/images/spotify_article/Jackson.png&quot; alt=&quot;Actual Handout&quot; /&gt;&lt;br /&gt;
Part of an &lt;em&gt;actual&lt;/em&gt; handout given to us by our electromagnetism professor (original by Davon Ferrara).&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/assets/images/spotify_article/Grading.jpg&quot; alt=&quot;Grading&quot; /&gt;&lt;br /&gt;
A selfie of the author grading tests with his classmates.&lt;/p&gt;

&lt;p&gt;I also really liked using it to explore genres that, despite listening to, I didn’t know existed.
For example, neo mellow was something I’d listened to quite a bit, yet I’d never heard of it before.
Selecting it on my dashboard shows me my top neo mellow artists…&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/assets/images/spotify_article/NeoMellow.png&quot; alt=&quot;Neo Mellow&quot; /&gt;&lt;/p&gt;

&lt;p&gt;and give me this fun moment where I thought to myself “now that you mention it, I kinda see what these guys have in common”.&lt;/p&gt;

&lt;p&gt;Now, I’m searching neo mellow playlists on Spotify and finding new things to listen to!&lt;/p&gt;

&lt;p&gt;My overall listening history also reminds me of &lt;a href=&quot;https://iopscience.iop.org/article/10.3847/1538-4357/ac20d7&quot;&gt;PSR J0218+4232&lt;/a&gt;, so that’s neat too. &lt;br /&gt;
&lt;img src=&quot;/assets/images/spotify_article/Pulsar.jpg&quot; alt=&quot;A pulsar in my thesis.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s really a lot to dig into, and I look forward to seeing other ways in which I use this dashboard in the future!&lt;/p&gt;

&lt;h1 id=&quot;technical-details&quot;&gt;Technical Details&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/limyansky/my_spotify&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;getting-the-data-from-spotify&quot;&gt;Getting the Data from Spotify&lt;/h2&gt;
&lt;p&gt;Spotify is constantly gathering data about how you interact with their services.
As a part of their data transparency policy, they will allow you to &lt;a href=&quot;https://support.spotify.com/us/article/data-rights-and-privacy-settings/&quot;&gt;request&lt;/a&gt; this data so that you know exactly what it is they are collecting.
The three types of user data you can download from Spotify are: account data, technical log information, and extended streaming history.
There’s actually quite a lot of data they collect, which you can read about on &lt;a href=&quot;https://support.spotify.com/us/article/understanding-my-data/&quot;&gt;their website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This project utilized only the “extended streaming history” data set.
This covers the lifetime of your account, and includes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Date/Time of Stream&lt;/li&gt;
  &lt;li&gt;Number of milliseconds track was played&lt;/li&gt;
  &lt;li&gt;Track name&lt;/li&gt;
  &lt;li&gt;Artist name&lt;/li&gt;
  &lt;li&gt;Album name&lt;/li&gt;
  &lt;li&gt;Track URI (Spotify’s unique identifier)&lt;/li&gt;
  &lt;li&gt;And More!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After requesting this data, it took about two weeks for it to be delivered.
They guarantee the data within 30 days of request.&lt;/p&gt;

&lt;p&gt;Most of the data I used in my dashboard was directly listed in the extended streaming history.
However, while I think genre of music best tells the story of my music tastes, this isn’t included in your extended history!
Which leads me to…&lt;/p&gt;

&lt;h2 id=&quot;the-spotify-api&quot;&gt;The Spotify API&lt;/h2&gt;
&lt;p&gt;If you aren’t familiar, API stands for “application programming interface”, and is a simple way for apps to talk to one another. For example, here’s an “API Endpoint” that asks the “REST Countries API” to give you information about Russia:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://restcountries.com/v3.1/name/Russia&quot;&gt;https://restcountries.com/v3.1/name/Russia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you instead want to know information about Germany, you can change the address to:&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://restcountries.com/v3.1/name/Germany&quot;&gt;https://restcountries.com/v3.1/name/Germany&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The idea is that your program is able to ask simple questions in this format (“Tell me about Germany”), and get an answer that is easy for the computer to understand and work with.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://developer.spotify.com/documentation/web-api&quot;&gt;Spotify API&lt;/a&gt; is free, but does require you to make a developer account and generate some credentials for yourself before you can use it. 
If you want a fun example of the kind of app you can build around this, check out &lt;a href=&quot;https://www.statsforspotify.com/&quot;&gt;statsforspotify.com&lt;/a&gt;.
After logging into your account, this website gives you details such as your top artists over a 4-week or 6-month time period, or the lifetime of your account. 
Why only these time periods?
Well, the Spotify API has a &lt;a href=&quot;https://developer.spotify.com/documentation/web-api/reference/get-users-top-artists-and-tracks&quot;&gt;get user’s top items&lt;/a&gt; function, which allows you to request a user’s most-streamed items over a &lt;code class=&quot;highlighter-rouge&quot;&gt;short_term&lt;/code&gt; (4 week), &lt;code class=&quot;highlighter-rouge&quot;&gt;medium_term&lt;/code&gt; (6 month) or &lt;code class=&quot;highlighter-rouge&quot;&gt;long_term&lt;/code&gt; (all) time period.
In fact, after taking a minute to dig through the API documentation, we can see that Stats for Spotify is a minimalist wrapper around the larger Spotify API!&lt;/p&gt;

&lt;p&gt;The downside to this, versus specifically downloading your data, is that your analysis is much more limited in scope.
While your extended streaming data includes a timestamp for every play of every song you’ve ever listened to, there’s really no way to request that information through the API.&lt;/p&gt;

&lt;p&gt;Anyways, that’s a broad overview of API’s in general.
In this project, I used the Spotify API to determine the genre of each song I had listened to. 
Specifically, I used the &lt;a href=&quot;https://spotipy.readthedocs.io/en/2.22.1/&quot;&gt;Spotipy&lt;/a&gt; python library to interact with the API.
I first took each song’s unique URI and requested the unique artist URI’s associated with that song (note: although the downloaded data contains artist name, this is a plaintext “Taylor Swift”, and insufficient to actually query the API).
Then, I requested the artist URI to request the genre’s associated with that artist. 
Correlating this data, I wound up with a list of genres associated with each song.&lt;/p&gt;

&lt;p&gt;There was a sneaky gottcha here - if a song was performed by, say, two “pop” artists, it gets two “pop” tags, and is counted in the genre total for “pop” twice! 
In SQL, you’d correct for this with a &lt;code class=&quot;highlighter-rouge&quot;&gt;GROUP BY&lt;/code&gt; command, but in Tableau I used a &lt;a href=&quot;https://help.tableau.com/current/pro/desktop/en-us/calculations_calculatedfields_lod.htm&quot;&gt;Level of Detail&lt;/a&gt; expression.
This is why you can select a genre on the dashboard, and the total will be different than the total for Hours by Artist.
While I was okay with saying “I listened to both Taylor Swift and Ed Sheeran for three minutes by listening to Everything has Changed”, I didn’t think it was fair to say I listened to six minutes of pop.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/assets/images/spotify_article/GenreHours.png&quot; alt=&quot;Genre and Artist hours differ&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 05 Feb 2024 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/Exploring-My-Lifetime-Spotify-History/</link>
        <guid isPermaLink="true">http://localhost:4000/Exploring-My-Lifetime-Spotify-History/</guid>
        
        
        <category>Data Analysis</category>
        
        <category>Tableau</category>
        
      </item>
    
      <item>
        <title>Classifying Personality with Machine Learning</title>
        <description>&lt;p&gt;In this project, I demonstrate a technique to classify a person’s Big 5 personality traits based off of 20-minute stream-of-consciousness essays.
I achieved an average accuracy of 58% across the five personality categories by using Sentence-BERT embeddings coupled with small neural network classifiers.
Although this is slightly less than the 60% accuracy achieved by other BERT-based techniques, my model represents a 1000x reduction in complexity from these other methods.
In addition to demonstrating the power of Sentence-BERT, this reduction in complexity is an important step in making these types of models more distributable to end users.
The code for this project is available on &lt;a href=&quot;https://github.com/limyansky/Personality-Classification/blob/main/Personality_Classification.ipynb&quot;&gt;GitHub&lt;/a&gt; as a Google Collaboratory notebook.&lt;/p&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;While I present this work in the context of psychology, this is a substantial deviation from the areas of research in which I have been formally trained.
This work has not been peer reviewed.
If these results seem relevant to your work, I’d instead point you to &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1110866521000311#b0225&quot;&gt;&lt;em&gt;Deep Learning Based Fusion Strategies for Personality Prediction&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Big_Five_personality_traits&quot;&gt;Big 5&lt;/a&gt; system classifies a person’s personality according to openness to experience, conscientiousness, extroversion, agreeableness, and neuroticism.
Where a person falls on this scale (and how this changes with time) can have implications over a wide range of topics, such as &lt;a href=&quot;https://archive.org/details/sim_personality-and-individual-differences_2011-09_51_4/page/n2/mode/1up&quot;&gt;academic achievement&lt;/a&gt; and &lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0272735803001132?via%3Dihub&quot;&gt;mental health&lt;/a&gt;.
Although the model isn’t without its &lt;a href=&quot;https://psycnet.apa.org/record/1995-21277-001&quot;&gt;criticism&lt;/a&gt;, studies relating to the Big 5 are prevalent in psychology.&lt;/p&gt;

&lt;p&gt;Typically, a person is assigned a personality score after responding to questions on a &lt;a href=&quot;https://openpsychometrics.org/tests/IPIP-BFFM/&quot;&gt;questionnaire&lt;/a&gt;.
However, there has been &lt;a href=&quot;https://link.springer.com/article/10.1007/s10462-019-09770-z&quot;&gt;considerable effort&lt;/a&gt; spent as of late towards developing alternate assessment techniques with deep learning.
It is to this task that I devote myself in this project: can a short essay from your stream-of-consciousness be used to predict your personality type?&lt;/p&gt;

&lt;p&gt;Throughout this project, I compared my work to that of &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1110866521000311&quot;&gt;El-Demerdash et. al.&lt;/a&gt; in their paper “Deep Learning Based Fusion Strategies for Personality Prediction”.
In this work, they attained 61.85% accuracy in determining a person’s personality type from their essays.
To achieve this, they used an ensemble of three large language models(LLMs), as well as a “fused” training dataset consisting of both the stream-of-consciousness essays and user’s facebook activity.
That is a more intensive project than I undertook, but as they recorded their intermediate results, the paper still serves as an apt point of comparison.&lt;/p&gt;

&lt;p&gt;In particular, they report the result of fine-tuning the BERT LLM on only the stream-of-consciousness data, after which they achieved 60.43% accuracy in personality classification.
This is most similar to my attempt, described further below, based around building light classifiers on top of Sentence-BERT embeddings.&lt;/p&gt;

&lt;h2 id=&quot;the-data&quot;&gt;The Data&lt;/h2&gt;
&lt;p&gt;The dataset used in this project consists of 2,467 essays collected by &lt;a href=&quot;https://psycnet.apa.org/doiLanding?doi=10.1037%2F0022-3514.77.6.1296&quot;&gt;James Pennebaker et al.&lt;/a&gt;.
Students wrote down their stream-of-consciousnesses, then took a standardized personality test.
Often referred to as the &lt;em&gt;Essays Dataset&lt;/em&gt;, this has become one of the most prominent datasets used to build and measure automated personality classification systems.
&lt;a href=&quot;https://sites.google.com/michalkosinski.com/mypersonality&quot;&gt;&lt;em&gt;myPersonality&lt;/em&gt;&lt;/a&gt;, a dataset consisting of Facebook content coupled with the user’s personality type information, is also quite prominent. 
However, this dataset’s maintainers stopped sharing it in 2018, and I felt it best to respect their withdrawal by not using it in this work.&lt;/p&gt;

&lt;h3 id=&quot;data-preparation&quot;&gt;Data Preparation&lt;/h3&gt;
&lt;p&gt;With five binary personality labels, there are a total of 32 unique personality types.
I used a &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html&quot;&gt;stratified shuffle split&lt;/a&gt; on these 32 categories to implement an 80/20 train-test split of the &lt;em&gt;Essays Dataset&lt;/em&gt;.
The training data was further divided, again using the stratified shuffle split, into an 80/20 train-valid datasets.
El-Demerdash et. al. used 90/10 splits, but given the small number of samples in this dataset, I felt the 80/20 ratio justified.&lt;/p&gt;

&lt;h2 id=&quot;bert&quot;&gt;BERT&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Bidirectional Encoder Representations from Transformers&lt;/em&gt;, or &lt;a href=&quot;https://arxiv.org/abs/1810.04805v2&quot;&gt;BERT&lt;/a&gt;, is a large language model (LLM) introduced by google in 2018.
BERT was originally &lt;a href=&quot;https://huggingface.co/bert-base-multilingual-cased&quot;&gt;trained&lt;/a&gt; to predict masked words in a sentence, and to tell if two sentences likely followed each other in a string of text.
However, it was created with the assumption that it would be fine-tuned to perform specific tasks, such as aiding &lt;a href=&quot;https://blog.google/products/search/search-language-understanding-bert/&quot;&gt;Google searches&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This project uses &lt;a href=&quot;https://arxiv.org/abs/1908.10084&quot;&gt;Sentence-BERT&lt;/a&gt;, a variation of BERT trained for semantic text similarity tasks.
Sentence-BERT takes a sentence or short paragraph as input, and outputs a point called an &lt;em&gt;embedding&lt;/em&gt; in 384-dimensional space.
The magic of Sentence-BERT is that these embeddings can be compared via &lt;a href=&quot;https://en.wikipedia.org/wiki/Cosine_similarity&quot;&gt;cosine similarity&lt;/a&gt; or Euclidean distance to determine if they convey similar meanings.&lt;/p&gt;

&lt;h2 id=&quot;my-method&quot;&gt;My Method&lt;/h2&gt;
&lt;p&gt;My classification technique was a two step process: embedding the paragraphs with Sentence-BERT, and then running these embeddings through binary classifiers.&lt;/p&gt;

&lt;h3 id=&quot;embedding&quot;&gt;Embedding&lt;/h3&gt;
&lt;p&gt;The version of Sentence-BERT I used, &lt;a href=&quot;https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2&quot;&gt;paraphrase-MiniLM-L6-v2&lt;/a&gt;, is limited to sequences of 128 &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/&quot;&gt;tokens&lt;/a&gt; (each token typically represents a few letters).
Concerned about running over this limit, I used Python’s &lt;a href=&quot;https://www.nltk.org/&quot;&gt;Natural Language Toolkit&lt;/a&gt; to split the paragraphs into individual sentences, ran the sentences through Sentence-BERT, then took the average of the resultant embeddings.
I refer to the average embedding as the &lt;em&gt;paragraph-level&lt;/em&gt; embedding.&lt;/p&gt;

&lt;p&gt;I experimented with principal component removal, in an analog &lt;a href=&quot;https://openreview.net/pdf?id=SyK00v5xx&quot;&gt;SIF-embedding&lt;/a&gt;.
While I find this technique very interesting, it did not considerably improve the performance of my classifiers.
I think this is consistent with the underlying justification for using SIF-embedding in the first place, but the topic is complex enough to warrant its own discussion.
Keep an eye out for a companion article from me in the future!
In the mean time, check out the accompanying notebook for further explanation.&lt;/p&gt;

&lt;h3 id=&quot;classification&quot;&gt;Classification&lt;/h3&gt;
&lt;p&gt;After forming paragraph-level embeddings for each essay, I tested three different classification techniques: a random forest, a dense neural network with a 50% reduction in neurons at each level (which I will refer to as 1/2-NN), and a dense neural network with a 75% reduction in neurons at each level (1/4-NN).&lt;/p&gt;

&lt;p&gt;Random forests are my favorite starting point for classification tasks; they are able to work on data that hasn’t been transformed (such as to have zero mean and unit variance), are resistant to over fitting, and can provide estimates on the relative importance of different features in the dataset.
While constructing this classifier, I left most of the parameters at their &lt;a href=&quot;https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html&quot;&gt;Scikit-Learn default&lt;/a&gt;.
Of particular importance, I drew with replacement the same number of samples which were in my training dataset, and randomly selected &lt;script type=&quot;math/tex&quot;&gt;\sqrt{384}&lt;/script&gt; features to generate each tree.
Using bootstrapping and multiple cores to speed calculation, I scanned ensembles of up to n=1000 trees, and used the out-of-bag samples as an estimator of model performance (however, in this regime of many features and few examples, the out-of-bag error rate is &lt;a href=&quot;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201904&quot;&gt;extremely pessimistic&lt;/a&gt;).
I eventually settled on an ensemble of 800 trees, after which I trained the final classifiers on a set of both the training and validation data.&lt;/p&gt;

&lt;p&gt;The neural network classifiers are composed of multiple dense layers. 1/2-NN consists of eight layers, with the structure 192-96-48-24-12-6-3-1 for a total of 98,683 trainable parameters.
1/4-NN consists of four layers, with the structure 96-24-6-1 for a total of 39,445 trainable parameters.
Both neural networks used the LeakyReLU activation function, and were initialized using &lt;a href=&quot;https://arxiv.org/abs/1502.01852&quot;&gt;He Normal Initialization&lt;/a&gt;.
The final output neuron of each network used a sigmoid activation function to produce a probability between 0 and 1.
I used the &lt;a href=&quot;http://cs229.stanford.edu/proj2015/054_report.pdf&quot;&gt;Nadam&lt;/a&gt; optimizer, with the step size determined by scanning a range of values, and choosing a size 1/10 the value where the minimum loss occurred (see the &lt;code class=&quot;highlighter-rouge&quot;&gt;find_learning_rate&lt;/code&gt; function in &lt;a href=&quot;https://github.com/ageron/handson-ml2/blob/master/11_training_deep_neural_networks.ipynb&quot;&gt;this&lt;/a&gt; notebook, meant to accompany &lt;a href=&quot;https://github.com/ageron/handson-ml2/blob/master/11_training_deep_neural_networks.ipynb&quot;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;).
This resulted in a step size &lt;script type=&quot;math/tex&quot;&gt;2 \times 10^{-4}&lt;/script&gt; of for 1/2-NN, and for &lt;script type=&quot;math/tex&quot;&gt;2 \times 10^{-3}&lt;/script&gt; for 1/4-NN.
Both neural networks were trained with early stopping, and weights were rolled-back to the point of lowest validation error.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/personality_class_results.JPG&quot; alt=&quot;Accuracy of different classification methods.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above table shows a comparison of the different classification techniques.
The last row is from the “BERT” row of Table 5 in &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1110866521000311&quot;&gt;El-Demerdash et. al.&lt;/a&gt;.
These authors present a considerably more exhaustive study, and were eventually able to achieve an average accuracy of 61.85%.
However, this involved more training data (fusing the Essays Dataset and myPersonality dataset), as well as an ensemble of three different classifiers, based on ELMo, ULMFiT, and BERT.
The included row in this table shows their results from fine-tuning only BERT on just the Essays Dataset.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/personality_class_size.JPG&quot; alt=&quot;Size of different classifiers.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above table shows the number of trainable parameters in each of the neural network approaches.
These numbers represent a single classifier, and there is one classifier per personality type.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;
&lt;p&gt;My neural network classifiers are able to achieve an average accuracy &amp;lt;3% less than current state of the art classifiers, while reducing the number of trainable parameters by 1000x-2700x.
This reduction in complexity is an important step towards making these models more accessible and easier to distribute.
As an anecdotal example, it is interesting to compare this project with my prior one on &lt;a href=&quot;https://limyansky.com/A-Guessing-Game-with-GPT2/&quot;&gt;fine-tuning GPT2 to generate scientific paper titles&lt;/a&gt;.
The version of GPT2 I used contained 124 million parameters, and fine-tuning took enough computational resources that I ran into the GPU usage limit on the free version of Google Collaboratory.
Training these smaller classifiers was considerably faster, and I never ran into usage restrictions (although, there are other factors at play here as well, such as different dataset sizes and me being more experienced).&lt;/p&gt;

&lt;p&gt;Combining Sentence-BERT with these smaller classifiers has additional benefits over fine-tuning the entire BERT model.
Because Sentence-BERT is standardized, the process of embedding a corpus of text can be more easily off-loaded to a remote, high-performance computing system, with the actual classification carried out on a local device.
Further towards this goal, a single fine-tuned BERT model takes up 1.3 GB of disk space, meaning the full 5 classifiers would require 6.5 GB to store.
In comparison, 1/2-NN requires up only 1.4 MB of disk space per model, and 1/4-NN requires a minuscule 644 kb per model.&lt;/p&gt;

&lt;p&gt;Finally, this project demonstrates the power of Sentence-BERT embeddings.
A model of BERT fine-tuned to classify “Openness” will only ever be able to classify openness, but the fact that this classification could so easily be pulled from Sentence-BERT embeddings implies that we are only just scratching the surface at what it is possible to achieve.&lt;/p&gt;

&lt;h2 id=&quot;ideas-for-the-future&quot;&gt;Ideas for the Future&lt;/h2&gt;
&lt;p&gt;One idea which particularly intrigues me is to look at a more complex methods of the embeddings of individual sentences into paragraphs.
For example, one could imagine building a model to classify individual sentences, then averaging the classification of each sentence in a paragraph to predict the personality type of the writer.
This could be further combined with clustering techniques to either remove outlier sentences, or perhaps even remove non-outliers.&lt;/p&gt;

</description>
        <pubDate>Sun, 23 Apr 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/Classifying-Personality-with-Machine-Learning/</link>
        <guid isPermaLink="true">http://localhost:4000/Classifying-Personality-with-Machine-Learning/</guid>
        
        
        <category>Machine Learning</category>
        
        <category>TensorFlow</category>
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>A Guessing Game with GPT2 and TensorFlow</title>
        <description>&lt;p&gt;In this project, I fine-tuned the GPT2 large language model (LLM) with TensorFlow so that it generates scientific-sounding paper titles for a real-or-fake guessing game.
You can view the project notebook (and run it in Google Colaboratory) &lt;a href=&quot;https://github.com/limyansky/GPT2_ArXiv_SnarXiv/blob/main/ArXiv_Any.ipynb&quot;&gt;here&lt;/a&gt;, and you can play the game by running this &lt;a href=&quot;https://colab.research.google.com/github/limyansky/GPT2_ArXiv_SnarXiv/blob/main/ArXiv_Play.ipynb&quot;&gt;interactive notebook&lt;/a&gt;.
The fake titles are generally realistic enough to pass cursory review, with the player needing subject-matter knowledge to determine real from fake.&lt;/p&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;
&lt;p&gt;LLMs in general, including the GPT family, are known to be &lt;a href=&quot;https://huggingface.co/gpt2&quot;&gt;biased&lt;/a&gt;, and occasionally generate offensive material.
I have not encountered any instances of this for this use case, but it is possible something has evaded my review.&lt;/p&gt;

&lt;h2 id=&quot;inspiration&quot;&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;During social hour in the physics department, we would occasionally play a guessing game called &lt;a href=&quot;http://snarxiv.org/vs-arxiv/&quot;&gt;ArXiv vs SnarXiv&lt;/a&gt;. 
ArXiv is an open access website where scientists post pre-print copies of their papers, while SnarXiv is a parody website which generates fake paper titles, abstracts, and authors for high-energy theory papers.
In this game, players are tasked with discerning which source (ArXiv or SnarXiv) each title originates from.
Although a fun way to pass the time, I never felt I could really give it an honest shot, as theoretical physics is not my area of expertise. 
Enter: GPT2.&lt;/p&gt;

&lt;p&gt;GPT stands for &lt;em&gt;Generative Pre-Trained Transformer&lt;/em&gt;. &lt;em&gt;Generative&lt;/em&gt;, because it auto-completes text, &lt;em&gt;pre-trained&lt;/em&gt; because this auto-complete task can be easily leveraged to perform other tasks, and finally &lt;em&gt;transformer&lt;/em&gt; refers to the underlying structure of the model.
I first heard of the GPT family of LLMs in relation to OpenAI’s &lt;a href=&quot;https://chat.openai.com/chat&quot;&gt;ChatGPT&lt;/a&gt;, an online chat-bot powered by the GPT3 model.
GPT2 is a bit older, but it is the most recent member of the family to be released as open-source. 
Through tweaking the GPT2 model in a process called “fine-tuning”, I thought I could get it to generate realistic sounding paper titles in a subject area that I am more familiar with.&lt;/p&gt;

&lt;h2 id=&quot;the-data&quot;&gt;The Data&lt;/h2&gt;

&lt;p&gt;The underlying data for this project comes from &lt;a href=&quot;https://arxiv.org/&quot;&gt;ArXiv.org&lt;/a&gt;.
ArXiv offers a place for scientists to upload their papers where they can be accessed by anyone, for free, without needing a university affiliation or publisher subscription.
It is ubiquitous in physics, with most recent papers also appearing on ArXiv.
Rather than scrape ArXiv myself, I used a &lt;a href=&quot;https://www.kaggle.com/datasets/Cornell-University/arxiv?resource=download&quot;&gt;Kaggle dataset&lt;/a&gt; containing the relevant information in a .json file.&lt;/p&gt;

&lt;p&gt;Originally, my plan was to create training sentences of the format:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;category: High Energy Physics - Experiment title: Building the LHC
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;for all 2+ million papers on ArXiv.
Then, using prompts of the format:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;category: High Energy Astrophysical Phenomena title: 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;GPT2 would end up generating a fake paper title.
By changing what occurred after the “category” statement, I could use this technique to generate titles from any given category, as well as from a combination of categories.&lt;/p&gt;

&lt;p&gt;Unfortunately, this approach led to me quickly surpassing the GPU usage limits in Google’s (free) Colaboratory notebooks.
I decided to simplify the task by picking a few categories of interest, selecting papers from only one category at a time, and fine-tuning a different model for each categorical selection.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Category&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Number of Papers&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Tissues and Organs&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1,992&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Materials Science&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;82,412&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;High Energy Physics - Experiment&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;49,629&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;High Energy Astrophysical Phenomena&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;50,447&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;While it took ~1 hr to perform a single epoch of training on the original 2+ million papers, training on 10’s of thousands of papers took only ~10-15 minutes per epoch, with “Tissues and Organs” being much faster.&lt;/p&gt;

&lt;p&gt;Data preparation was rather simple: I pulled the title, removed characters other than letters and numbers, and removed multiple spaces.
In hindsight, this was too conservative - the GPT2 vocabulary includes various forms of punctuation and special characters.
In particular, allowing the &lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt; symbol would really have improved the readability of my results.&lt;/p&gt;

&lt;h2 id=&quot;the-model&quot;&gt;The Model&lt;/h2&gt;
&lt;p&gt;Due to computational limitations, I opted to use the smallest version of &lt;a href=&quot;https://huggingface.co/gpt2&quot;&gt;GPT2&lt;/a&gt;, with 124 million trainable parameters. 
For comparison, &lt;a href=&quot;https://huggingface.co/gpt2-xl&quot;&gt;GPT2-XL&lt;/a&gt; has 1.5 billion, and &lt;a href=&quot;https://github.com/openai/gpt-3&quot;&gt;GPT3&lt;/a&gt; has 175 billion (GPT4 does not have a &lt;a href=&quot;https://www.mlyearning.org/gpt-4-parameters/&quot;&gt;disclosed number of parameters&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;LLM’s (and neural netowks in general) work with numbers, not raw text.
The process of converting text to numbers is called &lt;em&gt;tokenization&lt;/em&gt;, with each resultant token typically representing a few letters (a single word may be translated into multiple tokens).
An easy way to get a handle on this is to try out the online interface for the &lt;a href=&quot;https://platform.openai.com/tokenizer&quot;&gt;GPT3 tokenizer&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are a handful of special tokens which may exist in a tokenizer, such as &lt;em&gt;start of sequence&lt;/em&gt; (SOS), &lt;em&gt;end of sequence&lt;/em&gt; (EOS), and &lt;em&gt;padding&lt;/em&gt; (PAD).
The SOS and EOS tokens denote when an input starts and ends.
PAD tokens add extra characters to a set of sentences used in training, such that all sentences have the same length.
Through the use of an &lt;em&gt;attention mask&lt;/em&gt;, PAD tokens they are typically ignored during model training.&lt;/p&gt;

&lt;p&gt;GPT2 was only trained using EOS tokens, but I elected to add SOS and PAD tokens as well.
The addition of an SOS token was purely aesthetic - I prefer to ask GPT2 to auto-complete a sentence of the form
&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;|startoftext|&amp;gt;&lt;/code&gt;
as opposed to just an empty string.
The addition of a PAD token was necessary to ensure that GPT2 truncated the fake titles at a reasonable point, and without it (or by setting the EOS token to be the PAD token), GPT2 will generate text until it reaches a hard-coded limit.&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;
&lt;p&gt;As I mentioned earlier in this article, I used the free version of &lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Google Colaboratory&lt;/a&gt; (a.k.a. Colab) for this project, which offers free GPU and TPU time if resources are available.
Colab is meant for interactive tasks, meaning it has relatively conservative timeouts and usage restrictions.
As such, I opted to terminate training for most of my datasets after inspection showed that a single training epoch produced satisfactory results.
The exception to this was “Tissues and Organs”, which had much fewer examples. 
For this dataset, I performed multiple epochs of training, which included the implementation of early stopping. 
The final loss (cross-entropy with masking) of these models is shown in the table below.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Category&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Test Loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Tissues and Organs&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Materials Science&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.42&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;High Energy Physics - Experiment&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;High Energy Astrophysical Phenomena&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.37&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I didn’t perform hyperparameter tuning for this project.
I used optimizer settings from &lt;a href=&quot;https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html&quot;&gt;here&lt;/a&gt;, which worked well enough for my purposes.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;Note: I found that GPT2 would only capitalize the first letter of the title, while real authors had a variety of capitalization schemes.
In the game, I pass the real titles through the same text filtering as the training set, and lowercase all words.
It is also apparent where colons should be used, although the text filter removed them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some Real Titles&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt; Modeling Tumor Angiogenesis with Cellular Automata  

 Single photon detection performance of highly disordered NbTiN thin films  

 Multiboson production in W decays  

 Resonant micro-instabilities at quasi-parallel collisionless shocks cause or consequence of shock reformation  
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Some Fake Titles&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;A computational model of neural activity in mammalian brainstem structures  

All-electron spin oscillations at bulk FeMgO interfaces  

Long-lived proton-proton fusion particles of Pb-Pb collisions  

A short time scale model for gamma-ray bursts
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Overall, I was really pleased with these fake titles.
Here is a rough measure of my guessing performance after 20 rounds of each category:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Test&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;My Performance&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ArXiv vs SnarXiv&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;55%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Tissues and Organs&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;75%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Materials Science&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;65%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;High Energy Physics - Experiment&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;55%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;High Energy Astrophysical Phenomena&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;75%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Not awful, considering my prior experiences with the base ArXiv vs SnarXiv game. 
Based off of this small sample, the “Tissues and Organs” category had the most poorly trained model, producing strange or grammatically incorrect titles:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;all patients with chronic liver disease

s a monozygotic deletion of chromosome 7 deletion in the cretylovus
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To be honest, I’m somewhat embarrassed with my performance in the astrophysics category, and think I should have been able to score much better if I was a bit more careful.
However, this was the last category I tried, meaning I’d already read 160 real and fake titles by the time I’d gotten there. 
I think I was a little tired out by that point…&lt;/p&gt;

&lt;h2 id=&quot;reflections-on-humor&quot;&gt;Reflections on Humor&lt;/h2&gt;

&lt;p&gt;I think this excerpt from &lt;em&gt;The Onion&lt;/em&gt;’s &lt;a href=&quot;https://www.supremecourt.gov/DocketPDF/22/22-293/242292/20221003125252896_35295545_1-22.10.03%20-%20Novak-Parma%20-%20Onion%20Amicus%20Brief.pdf&quot;&gt;amicus brief&lt;/a&gt; explaining their motto &lt;em&gt;Tu stultus es&lt;/em&gt; does an excellent job explaining why I had so much fun with this project (if you only read one Amicus brief in your life, I highly suggest it be this one):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…the phrase “you are dumb” captures
the very heart of parody: tricking readers into believing that they’re seeing a serious rendering of some specific form—a pop song lyric, a newspaper article, a
police beat—and then allowing them to laugh at their
own gullibility when they realize that they’ve fallen
victim to one of the oldest tricks in the history of rhetoric.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;One of parody’s most powerful
capacities is rhetorical: It gives people the ability to
mimic the voice of a serious authority—whether that’s
the dry news-speak of the Associated Press or the legalese of a court’s majority opinion—and thereby kneecap the authority from within.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Of course, I’m not fighting against any authority here, but I got a lot of enjoyment from watching this model poke fun of my field/knowledge.
For example, consider: 
&lt;code class=&quot;highlighter-rouge&quot;&gt;J1305-4420-4533 a strange globular cluster I an enigmatic low-mass black hole mass hadrons and spin oscillations
&lt;/code&gt;
Astronomy does use a naming convention that looks like &lt;code class=&quot;highlighter-rouge&quot;&gt;PSR J1846-0285&lt;/code&gt;, where ‘PSR’ refers to the object type, ‘J’ refers to a particular coordinate system, and ‘1846-0285’ are the coordinates in that system.
Adding a third set of coordinates to a title like this is gibberish - but, then again, this naming scheme certainly look like gibberish to a lot of people anyways.&lt;/p&gt;

&lt;p&gt;Interestingly, I also realized it was possible for GPT to do &lt;em&gt;too good&lt;/em&gt; a job of generating fake paper titles.
Take this example from chatGPT:&lt;br /&gt;
&lt;img src=&quot;/assets/images/chatGPT_SnarXiv.JPG&quot; alt=&quot;Asking chatGPT to make some fake paper titles.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Googling the first of these titles proves that it is, in fact, made up.
But, take at look at the very-real “&lt;a href=&quot;https://academic.oup.com/mnras/article/507/3/4564/6353538&quot;&gt;Gamma-ray emission from young radio galaxies and quasars&lt;/a&gt;” or “&lt;a href=&quot;https://aasnova.org/2019/03/18/missing-halos-in-the-high-energy-sky/&quot;&gt;Missing Halos in the High-Energy Sky&lt;/a&gt;”.
The generated title “Exploring the High-Energy Gamma-Ray Emission from AGN Jets” could conceivably be an alternate title for these works.
Where’s the fun in that?
The joy comes from the nonsense!&lt;/p&gt;

&lt;h2 id=&quot;thanks&quot;&gt;Thanks!&lt;/h2&gt;
&lt;p&gt;I hope you enjoyed reading about this project as much as I enjoyed working on it.
I’m open to any and all feedback!&lt;/p&gt;

&lt;h2 id=&quot;edits&quot;&gt;Edits&lt;/h2&gt;
&lt;p&gt;I may occasionally update this page.
For the full version history, please refer to &lt;a href=&quot;https://github.com/limyansky/personal_website/commits/main/_posts/2023-03-23-A-Guessing-Game-with-GPT2.md&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://huggingface.co/blog/how-to-generate&quot;&gt;https://huggingface.co/blog/how-to-generate&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/#3-text-generation-with-gpt-2&quot;&gt;https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/#3-text-generation-with-gpt-2&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.kaggle.com/code/ysthehurricane/text-generation-with-gpt2-huggingface#GPT2-Tokenizer&quot;&gt;https://www.kaggle.com/code/ysthehurricane/text-generation-with-gpt2-huggingface#GPT2-Tokenizer&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://hyunjoonlee70.github.io/Blog_Post_3/&quot;&gt;https://hyunjoonlee70.github.io/Blog_Post_3/&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d&quot;&gt;https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912#:~:text=GPT%2D2%20comes%20in%204,and%201.5B%20parameters%2C%20respectively&quot;&gt;https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912#:~:text=GPT%2D2%20comes%20in%204,and%201.5B%20parameters%2C%20respectively&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://huggingface.co/course/chapter7/&quot;&gt;https://huggingface.co/course/chapter7/&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://huggingface.co/course/chapter3/&quot;&gt;https://huggingface.co/course/chapter3/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 23 Mar 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/A-Guessing-Game-with-GPT2/</link>
        <guid isPermaLink="true">http://localhost:4000/A-Guessing-Game-with-GPT2/</guid>
        
        
        <category>Machine Learning</category>
        
        <category>TensorFlow</category>
        
        <category>GPT</category>
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Using MySQL while Studying Data Science</title>
        <description>&lt;!-- 
SQL puts the &quot;Data&quot; in &quot;Data Science&quot;.
Can you do science without data?
Sure, but we call those people &quot;Theorists&quot;, and if you're reading this you're either not interested in, or fleeing from, such a destitute lifestyle. --&gt;

&lt;p&gt;SQL is a programming language used to access information stored on a special type of database called a &lt;em&gt;relational&lt;/em&gt; database.
In a nutshell, relational databases offer an easy way to tie together clusters of information.
For example, a school database may consist of three tables: “Student Information”, “Skydiving Club”, and “Flying Club”.
SQL provides an easy way to, say, find students in “Skydiving Club” with over 100 jumps, cross reference that with students having over 100 flight hours in “Flying Club”, and look up their emails in “Student Information” so that they can be congratulated.
Data science makes heavy use of relational databases, and thus SQL.&lt;/p&gt;

&lt;p&gt;Using SQL to query a database isn’t a particularly arduous task, but it does involve a specific syntax that is best studied through repetition.
As such, I thought it best to set up a database early in my studies.
By using this to store my practice data, I’ll be getting SQL experience as I follow along with other portions of my study material.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;
&lt;p&gt;SQL comes in many different &lt;a href=&quot;https://towardsdatascience.com/how-to-find-your-way-through-the-different-types-of-sql-26e3d3c20aab&quot;&gt;dialects&lt;/a&gt;, which are slight variations of the language.
I have selected MySQL, as it seems reasonably popular, has a nice GUI that I find helpful for learning, and is one of the dialects available on &lt;a href=&quot;hackerrank.com&quot;&gt;HackerRank&lt;/a&gt; (a website often used to test programming skills in job interviews).&lt;/p&gt;

&lt;p&gt;To practice with SQL, you will need&lt;/p&gt;

&lt;p&gt;My SQL website
Anaconda python package&lt;/p&gt;

&lt;h2 id=&quot;making-a-new-database&quot;&gt;Making a New Database&lt;/h2&gt;

&lt;h2 id=&quot;accessing-with-python&quot;&gt;Accessing with Python&lt;/h2&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;W3, or a cheat sheet&lt;/p&gt;

&lt;h2 id=&quot;my-databases&quot;&gt;My Databases&lt;/h2&gt;

&lt;h2 id=&quot;insights-from-astrophysics&quot;&gt;Insights from Astrophysics&lt;/h2&gt;
&lt;p&gt;Similarities to fselect and&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Sep 2022 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/Studying-with-MySQL/</link>
        <guid isPermaLink="true">http://localhost:4000/Studying-with-MySQL/</guid>
        
        
        <category>Data Science</category>
        
        <category>Studying</category>
        
        <category>SQL</category>
        
      </item>
    
      <item>
        <title>My Thesis: Six Years in Six Minutes</title>
        <description>
</description>
        <pubDate>Fri, 30 Sep 2022 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/My-Thesis-Six-Years-in-Six-Minutes/</link>
        <guid isPermaLink="true">http://localhost:4000/My-Thesis-Six-Years-in-Six-Minutes/</guid>
        
        
        <category>Projects</category>
        
      </item>
    
      <item>
        <title>Exploring Aircraft Accidents</title>
        <description>&lt;p&gt;After going through some tutorials on W3 to learn SQL, it quickly became apparent that the best way to learn this language would be to download some datasets I was interested in and have at it.
As I have an interest in aviation, my thought immediately jumped to looking at aircraft accident statistics.
Most serious or semi-serious accidents are reported to the National Transportation and Safety Board, or NTSB.
Conveniently, the NTSB provides their database of accidents since 1982 as a ~400 MB Microsoft Access .mdb file.&lt;/p&gt;

&lt;p&gt;Overview of how hours are often used as a proxy for experience&lt;/p&gt;

&lt;p&gt;Originally, it was my hope to test anecdotal claims I’ve come across, such as “The accident rate decreases after a pilot gets 100 hours” and “The accident rate increases again at 200 hours due to overconfidence”.
As we will see, these questions aren’t really answerable using this set of data.
However, here’s some cool stuff I found.&lt;/p&gt;

&lt;h2 id=&quot;population-domination-example&quot;&gt;Population Domination Example&lt;/h2&gt;
&lt;p&gt;Simply put, this data set appears to be dominated by the relative population of pilots and aircraft.
For example, consider the top 10 most accident-prone aircraft:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;SELECT acft_model, COUNT(acft_model) AS tot_incidents
FROM aircraft
GROUP BY acft_model
ORDER BY tot_incidents DESC
LIMIT 10;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some output table here&lt;/p&gt;

&lt;p&gt;It appears that the Cessna C-172 features strongly in this list, although it would be wise to consider all variations of the aircraft (The 172, 172S, and 172M should all be considered the same type of aircraft in this scenario)&lt;/p&gt;

&lt;h2 id=&quot;cleaning-the-data&quot;&gt;Cleaning the Data&lt;/h2&gt;

&lt;p&gt;Lets look at the top ten aircraft models with the most incident reports.&lt;/p&gt;

&lt;p&gt;Immediately we run into issues.&lt;/p&gt;

&lt;p&gt;Look at the Cessna 172&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;SELECT acft_model
FROM `aircraft`
WHERE acft_model LIKE '%172%'
GROUP BY acft_model;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have entries for “172 M” “CE-172M” “172 - M” which are are all referring to the same model of aircraft.&lt;/p&gt;

&lt;p&gt;We have similar problems for aircraft make&lt;/p&gt;

&lt;p&gt;Solve this problem using lookup tables&lt;/p&gt;

&lt;p&gt;Every time I run this sql command, my gui crashers. But, when I reload, the table is there, so…&lt;/p&gt;

&lt;p&gt;Congrats to Thomas C. Piper on their homebuilt Murphy Rebel&lt;/p&gt;

&lt;p&gt;Boeing-Stearman: long history of purchases and sales&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Sep 2022 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/Exploring-Aircraft-Accidents-with-SQL/</link>
        <guid isPermaLink="true">http://localhost:4000/Exploring-Aircraft-Accidents-with-SQL/</guid>
        
        
        <category>Data Science</category>
        
        <category>Studying</category>
        
        <category>SQL</category>
        
      </item>
    
      <item>
        <title>Building a Website: A Physicist's Perspective</title>
        <description>&lt;p&gt;A common piece of advice for those transitioning from academia to industry is to build a personal portfolio of projects to show off your skills and talents.
One of the first elements to building such a portfolio is deciding how to display it.
In creating my own website, I sought something cheap and easy to write, but also over which I had a large degree of creative control.
I also thought it would be fun if I could use my own domain name.
These requirements pushed me towards &lt;a href=&quot;https://domains.google.com&quot;&gt;Google Domains&lt;/a&gt; to manage my domain name, &lt;a href=&quot;https://pages.github.com&quot;&gt;GitHub Pages&lt;/a&gt; to host my website, and &lt;a href=&quot;https://www.jekyllrb.com&quot;&gt;jekyll&lt;/a&gt; for formatting.&lt;/p&gt;

&lt;p&gt;In this blog post, I will discuss how I came to this final setup, as well as various prior attempts at website generation.
I consider this “A Physicist’s Perspective” because my ultimate decision makes use of skills I’ve acquired during my time in academia, specifically GitHub, Linux (optional, but command line is a must), and Markdown, as well as an existing Google account.
I have opted not to make this post a specific tutorial, and will mostly focus on providing an overview of my setup.
If you are interested in a tutorial, I recommend looking at GitHubs &lt;a href=&quot;https://docs.github.com/en/pages&quot;&gt;official documentation&lt;/a&gt;.
This will walk you through everything I discuss below, including using jekyll to build web content and write blog posts.&lt;/p&gt;

&lt;h2 id=&quot;building-and-hosting-a-website&quot;&gt;Building and Hosting a Website&lt;/h2&gt;
&lt;p&gt;For amateur setups, the choice of tools used to build and host your website are intimately intertwined.
Some hosting providers will require you to use a specific piece software to build your website, while others have certain technical requirements that won’t work with particular development software.
If you have a strong preference in regards to software or web host, I recommend starting there and finding a complimentary service that will work well your choice.&lt;/p&gt;

&lt;h3 id=&quot;hosting&quot;&gt;Hosting&lt;/h3&gt;
&lt;p&gt;I knew almost immediately that I wanted to host my website on GitHub Pages.
This is a free service provided by GitHub that will host any of your public repositories as websites.
It even gives you the option of connecting a domain name that you already own!&lt;/p&gt;

&lt;p&gt;For someone already familiar with GitHub, selecting a repository branch to be served as a website involves changing only a single setting.
Managing the website is done through exactly the same procedures as managing any other repository.&lt;/p&gt;

&lt;p&gt;However, in my personal experience, GitHub has a steep enough learning curve that getting it set up exclusively to host a website may not be the best option for everyone.
My limited experience with &lt;a href=&quot;https://wordpress.com&quot;&gt;WordPress.com&lt;/a&gt; showed it to be a more user-friendly experience, although this comes at the cost of, well, money.&lt;/p&gt;

&lt;h3 id=&quot;building&quot;&gt;Building&lt;/h3&gt;
&lt;p&gt;GitHub Pages limits you to hosting &lt;em&gt;static&lt;/em&gt; websites.
These are websites that provide identical content for all users, such as what one typically expects of a personal blog.
Static websites are contrasted with &lt;em&gt;dynamic&lt;/em&gt; websites, where content is generated via an app on a user-by-user basis.
Most online retailers will use dynamic websites, which build their webpages by querying databases for the products most likely to appeal to each specific user.&lt;/p&gt;

&lt;p&gt;The WordPress software suite, although free and open source, natively generates dynamic websites that won’t work with GitHub Pages.
There are some workarounds, such as the SimplyStatic plugin, but I had trouble getting these tools to work properly.&lt;/p&gt;

&lt;p&gt;I also considered the bare-bones option of writing the website in raw HTML and CSS.
I found this to be the simplest path forward, as it doesn’t involve tinkering with any software other than a text editor.
However, your website will inevitably end up looking like it is from the 90’s.
My HTML/CSS attempt is still alive at &lt;a href=&quot;https://limyansky.github.io/&quot;&gt;limyansky.github.io&lt;/a&gt; (hey, its free!), for all those interested in taking a look.&lt;/p&gt;

&lt;p&gt;After this trial and error, I settled on using jekyll to build my website.
This software is built with Ruby, and easily translates &lt;a href=&quot;https://www.markdownguide.org/&quot;&gt;Markdown&lt;/a&gt; documents into decently looking websites.
Like any good tool, the sky’s the limit!
For examples of what the pros can build jekyll, take a look at &lt;a href=&quot;https://developer.spotify.com/&quot;&gt;Spotify for Developers&lt;/a&gt; and the website for the &lt;a href=&quot;https://www.foia.gov/&quot;&gt;Freedom of Information Act&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For my purposes, I’m already familiar with Markdown via my use of &lt;a href=&quot;https://obsidian.md/&quot;&gt;Obsidian&lt;/a&gt; as a note taking app in academia, which makes writing new blog posts a breeze.
Jekyll is also explicitly mentioned by GitHub as working well with Pages, with the GitHub team going as far as including specific tutorials for certain aspects of its use.
Although I have no prior experience with Ruby, which did cause some confusion during setup, jekyll is popular enough that tutorials and StackExchange questions are plentiful.&lt;/p&gt;

&lt;h2 id=&quot;a-note-on-domain-names&quot;&gt;A Note on Domain Names&lt;/h2&gt;
&lt;p&gt;Registering a domain name is a comparatively simple process - you find a domain registrar (Google Domains and NameCheap are popular examples) and pay them a few dollars to claim your desired name.
There may be a few settings to tweak when connecting your domain name to your hosting service, but tutorials are abundant from registrars and hosting providers.&lt;/p&gt;

&lt;p&gt;However, something very important to keep in mind is that missing a payment (although most services offer a grace period) on your domain name means you are likely to lose access to the name for a number of years.
Rather than simply being de-listed, old domain names are often auctioned to investors looking either to resell them for a profit or make money from their existing user base. 
One organization I was previously involved with lost their domain in this manner, although it was eventually de-listed about two years later.&lt;/p&gt;

&lt;p&gt;All this is to say that I chose Google Domains not only because I found their pricing competitive ($12 at the time of writing), but because I knew I could keep up to date with the payments if I associated them with my existing google account.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;I found the most challenging part of making this website to be selecting software and providers that would work well together.
I hope that by going into detail on what I finally settled on, I can save you some time in designing your own website.
Good luck!&lt;/p&gt;
</description>
        <pubDate>Tue, 27 Sep 2022 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/Building-A-Website/</link>
        <guid isPermaLink="true">http://localhost:4000/Building-A-Website/</guid>
        
        
        <category>project</category>
        
        <category>github</category>
        
      </item>
    
  </channel>
</rss>
